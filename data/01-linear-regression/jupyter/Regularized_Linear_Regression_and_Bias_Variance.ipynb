{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Online Class\n",
    "##  Exercise 5 | Regularized Linear Regression and Bias-Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy and pandas, and DataFrame / Series\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as optimize\n",
    "import math\n",
    "from __future__ import division\n",
    "from pandas import DataFrame, Series\n",
    "    \n",
    "# Set some pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# And some items for %matplotlib\n",
    "%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "#plt.rc('font', family='sans-serif') \n",
    "#plt.rc('font', serif='Helvetica Neue') \n",
    "#plt.rc('text', usetex='false') \n",
    "#plt.rcParams.update({'font.size': 22})\n",
    "#http://stackoverflow.com/questions/33995707/attributeerror-unknown-property-color-cycle\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 1: Loading and Visualizing Data ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "print ('Loading and Visualizing Data ...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = np.loadtxt('linearRegData/X.csv', delimiter=',')\n",
    "#y = np.loadtxt('linearRegData/y.csv', delimiter=',')\n",
    "#Xval = np.loadtxt('linearRegData/Xval.csv', delimiter=',')\n",
    "#yval = np.loadtxt('linearRegData/yval.csv', delimiter=',')\n",
    "#Xtest = np.loadtxt('linearRegData/Xtest.csv', delimiter=',')\n",
    "#ytest = np.loadtxt('linearRegData/ytest.csv', delimiter=',')\n",
    "X = pd.read_csv('linearRegData/X.csv', dtype=float, header=None)\n",
    "y = pd.read_csv('linearRegData/y.csv', dtype=float, header=None)\n",
    "Xval = pd.read_csv('linearRegData/Xval.csv', dtype=float, header=None)\n",
    "yval = pd.read_csv('linearRegData/yval.csv', dtype=float, header=None)\n",
    "Xtest = pd.read_csv('linearRegData/Xtest.csv', dtype=float, header=None)\n",
    "ytest = pd.read_csv('linearRegData/ytest.csv', dtype=float, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#number of examples\n",
    "m=X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b+', markersize=10, linewidth=1.5);\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 2: Regularized Linear Regression Cost ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linearRegCostFunctionCombo(X, y, theta, lamda):\n",
    "#LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear \n",
    "#regression with multiple variables\n",
    "#   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the \n",
    "#   cost of using theta as the parameter for linear regression to fit the \n",
    "#   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0;\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # Instructions: Compute the cost and gradient of regularized linear \n",
    "    #               regression for a particular choice of theta.\n",
    "    #\n",
    "    #               You should set J to the cost and grad to the gradient.\n",
    "    #             You should set J to the cost.\n",
    "\n",
    "    d = ((theta.T.dot(X.T)).T-y)\n",
    "    d = d * d;\n",
    "\n",
    "    # See http://stackoverflow.com/questions/15988832/how-do-you-unroll-a-numpy-array-of-mxn-dimentions-into-a-single-vector\n",
    "    theta_unrolled = theta.T.ravel()\n",
    "    J = (1.0/(2.0*m))*d.sum()+(lamda/(2.0*m))*((theta_unrolled[1:]*theta_unrolled[1:]).sum())\n",
    "    \n",
    "    r = (lamda/(1.0*m))*(theta)\n",
    "    r[0]=0\n",
    "    \n",
    "    grad = (((X.dot(theta)-y).T.dot(X))/m).as_matrix() + r.T\n",
    "    \n",
    "    grad = (grad.T).ravel()\n",
    "\n",
    "    return (J, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegCostFunction(theta_unraveled, X, y, lamda):\n",
    "#LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear \n",
    "#regression with multiple variables\n",
    "#   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the \n",
    "#   cost of using theta as the parameter for linear regression to fit the \n",
    "#   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0;\n",
    "    \n",
    "    # Instructions: Compute the cost and gradient of regularized linear \n",
    "    #               regression for a particular choice of theta.\n",
    "    #\n",
    "    #               You should set J to the cost and grad to the gradient.\n",
    "    #             You should set J to the cost.\n",
    "    theta = np.reshape(theta_unraveled, (theta_unraveled.shape[0], 1))\n",
    "    d = ((theta.T.dot(X.T)).T-y)\n",
    "    d = d * d;\n",
    "\n",
    "    # See http://stackoverflow.com/questions/15988832/how-do-you-unroll-a-numpy-array-of-mxn-dimentions-into-a-single-vector\n",
    "    #theta_unrolled = theta.T.ravel()\n",
    "    J = (1.0/(2.0*m))*d.sum()+(lamda/(2.0*m))*((theta_unraveled[1:]*theta_unraveled[1:]).sum())\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegCostGradFunction(theta_unraveled, X, y, lamda):\n",
    "#LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear \n",
    "#regression with multiple variables\n",
    "#   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the \n",
    "#   cost of using theta as the parameter for linear regression to fit the \n",
    "#   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "\n",
    "    theta = np.reshape(theta_unraveled, (theta_unraveled.shape[0], 1))\n",
    "\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    r = (lamda/(1.0*m))*(theta)\n",
    "    r[0]=0\n",
    "    \n",
    "    grad = (((X.dot(theta)-y).T.dot(X))/m).as_matrix() + r.T\n",
    "    \n",
    "    grad = (grad.T).ravel()\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newX = np.hstack((np.ones((m, 1)), X))\n",
    "theta = np.ones(newX.shape[1])\n",
    "J = linearRegCostFunction(theta, newX, y, 1)\n",
    "grad = linearRegCostGradFunction(theta, newX, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (('Cost at theta = [1 ; 1]: %f '\n",
    "       '\\n(this value should be about 303.993192)\\n') % (J))\n",
    "print (type(newX))\n",
    "print (type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 3: Regularized Linear Regression Gradient ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (('Gradient at theta = [1 ; 1]:  [%f; %f] '\n",
    "       '\\n(this value should be about [-15.303016; 598.250744])\\n' ) % (grad[0], grad[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 4: Train Linear Regression ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modified from\n",
    "# http://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy\n",
    "def trainLinearReg(X, y, lamda):\n",
    "    #%TRAINLINEARREG Trains linear regression given a dataset (X, y) and a\n",
    "    #%regularization parameter lambda\n",
    "    #%   [theta] = TRAINLINEARREG (X, y, lambda) trains linear regression using\n",
    "    #%   the dataset (X, y) and regularization parameter lambda. Returns the\n",
    "    #%   trained parameters theta.\n",
    "    #%\n",
    "\n",
    "    # Initialize Theta\n",
    "    initial_theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    Result = optimize.minimize(fun = linearRegCostFunction, x0 = initial_theta, args = (X, y, lamda), method = 'TNC',\n",
    "                                 jac = linearRegCostGradFunction);\n",
    "    optimal_theta = Result.x;\n",
    "    \n",
    "    return optimal_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Train linear regression with lambda = 0\n",
    "lamda = 0;\n",
    "theta = trainLinearReg(newX, y, lamda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Plot fit over the data\n",
    "plt.plot(X, y, 'rx', markersize=10, linewidth=1.5);\n",
    "plt.xlabel('Change in water level (x)');\n",
    "plt.ylabel('Water flowing out of the dam (y)');\n",
    "plt.plot(X, newX.dot(theta), '--', linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 5: Learning Curve for Linear Regression ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function [error_train, error_val] = ...\n",
    "def learningCurve(X, y, Xval, yval, lamda):\n",
    "#%LEARNINGCURVE Generates the train and cross validation set errors needed \n",
    "#%to plot a learning curve\n",
    "#%   [error_train, error_val] = ...\n",
    "#%       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and\n",
    "#%       cross validation set errors for a learning curve. In particular, \n",
    "#%       it returns two vectors of the same length - error_train and \n",
    "#%       error_val. Then, error_train(i) contains the training error for\n",
    "#%       i examples (and similarly for error_val(i)).\n",
    "#%\n",
    "#%   In this function, you will compute the train and test errors for\n",
    "#%   dataset sizes from 1 up to m. In practice, when working with larger\n",
    "#%   datasets, you might want to do this in larger intervals.\n",
    "#%\n",
    "\n",
    "    #% Number of training examples\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m)\n",
    "    error_val   = np.zeros(m)\n",
    "\n",
    "#% ====================== YOUR CODE HERE ======================\n",
    "#% Instructions: Fill in this function to return training errors in \n",
    "#%               error_train and the cross validation errors in error_val. \n",
    "#%               i.e., error_train(i) and \n",
    "#%               error_val(i) should give you the errors\n",
    "#%               obtained after training on i examples.\n",
    "#%\n",
    "#% Note: You should evaluate the training error on the first i training\n",
    "#%       examples (i.e., X(1:i, :) and y(1:i)).\n",
    "#%\n",
    "#%       For the cross-validation error, you should instead evaluate on\n",
    "#%       the _entire_ cross validation set (Xval and yval).\n",
    "#%\n",
    "#% Note: If you are using your cost function (linearRegCostFunction)\n",
    "#%       to compute the training and cross validation error, you should \n",
    "#%       call the function with the lambda argument set to 0. \n",
    "#%       Do note that you will still need to use lambda when running\n",
    "#%       the training to obtain the theta parameters.\n",
    "#%\n",
    "#% Hint: You can loop over the examples with the following:\n",
    "#%\n",
    "#%       for i = 1:m\n",
    "#%           % Compute train/cross validation errors using training examples \n",
    "#%           % X(1:i, :) and y(1:i), storing the result in \n",
    "#%           % error_train(i) and error_val(i)\n",
    "#%           ....\n",
    "#%           \n",
    "#%       end\n",
    "#%\n",
    "\n",
    "#% ---------------------- Sample Solution ----------------------\n",
    "    for i in np.arange(1,m+1):\n",
    "        Xtrain = X[0:i,:]\n",
    "        ytrain = y[0:i]\n",
    "        theta = trainLinearReg(Xtrain, ytrain, lamda);\n",
    "\n",
    "        trainError = linearRegCostFunction(theta, Xtrain, ytrain, 0)\n",
    "        #trainGrad = linearRegCostGradFunction(theta, Xtrain, ytrain, 0)\n",
    "        valError = linearRegCostFunction(theta, Xval, yval, 0)\n",
    "        #valGrad = linearRegCostGradFunction(theta, Xval, yval, 0)\n",
    "\n",
    "        error_train[i-1] = trainError;\n",
    "        error_val[i-1] = valError;\n",
    "\n",
    "    return (error_train, error_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lamda = 0;\n",
    "newXval = np.hstack((np.ones((Xval.shape[0], 1)), Xval))\n",
    "(error_train, error_val) = learningCurve(newX, y, newXval, yval, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(np.arange(1,m+1)), error_train, np.array(np.arange(1,m+1)), error_val);\n",
    "plt.title('Learning curve for linear regression')\n",
    "plt.legend(('Train', 'Cross Validation'))\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Error')\n",
    "#axis([0 13 0 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('# Training Examples\\tTrain Error\\tCross Validation Error\\n')\n",
    "for i in np.arange(0,m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f\\n'% (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 6: Feature Mapping for Polynomial Regression ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polyFeatures(X, p):\n",
    "#%POLYFEATURES Maps X (1D vector) into the p-th power\n",
    "#%   [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and\n",
    "#%   maps each example into its polynomial features where\n",
    "#%   X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^p];\n",
    "#%\n",
    "\n",
    "    #% You need to return the following variables correctly.\n",
    "    X_poly = pd.DataFrame(np.zeros((X.shape[0]*X.shape[1], p)))\n",
    "\n",
    "    #% ====================== YOUR CODE HERE ======================\n",
    "    #% Instructions: Given a vector X, return a matrix X_poly where the p-th \n",
    "    #%               column of X contains the values of X to the p-th power.\n",
    "    #%\n",
    "    #% \n",
    "\n",
    "    for i in np.arange(p):\n",
    "        X_poly.iloc[:,i] = X ** (i+1)                        \n",
    "\n",
    "    return X_poly.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 8;\n",
    "\n",
    "# Map X onto Polynomial Features\n",
    "X_poly = polyFeatures(X, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    #FEATURENORMALIZE Normalizes the features in X \n",
    "    #   FEATURENORMALIZE(X) returns a normalized version of X where\n",
    "    #   the mean value of each feature is 0 and the standard deviation\n",
    "    #   is 1. This is often a good preprocessing step to do when\n",
    "    #   working with learning algorithms.        \n",
    "    mu = X.mean(axis=0)\n",
    "    sigma = X.std(axis=0)\n",
    "    numerator = X - ((np.ones( (X.shape[0],X.shape[1]) ) * mu ))\n",
    "    denominator = ((np.ones( (X.shape[0],X.shape[1]) ) * sigma ))\n",
    "\n",
    "    X_norm = numerator / denominator\n",
    "\n",
    "    return (X_norm, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "(X_poly, mu, sigma) = featureNormalize(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add intercept term to X\n",
    "X_poly = np.hstack((np.ones((X.shape[0],1)), X_poly)) # Add Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p)\n",
    "X_poly_test = (X_poly_test - mu)/sigma\n",
    "X_poly_test = np.hstack((np.ones((X_poly_test.shape[0],1)), X_poly_test)) # Add Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p)\n",
    "X_poly_val = (X_poly_val - mu)/sigma\n",
    "X_poly_val = np.hstack((np.ones((X_poly_val.shape[0],1)), X_poly_val)) # Add Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Normalized Training Example 1:\\n')\n",
    "print  (X_poly[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 7: Learning Curve for Polynomial Regression =============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lamda = 0;\n",
    "theta = trainLinearReg(X_poly, y, lamda);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotFit(min_x, max_x, mu, sigma, theta, p):\n",
    "#%PLOTFIT Plots a learned polynomial regression fit over an existing figure.\n",
    "#%Also works with linear regression.\n",
    "#%   PLOTFIT(min_x, max_x, mu, sigma, theta, p) plots the learned polynomial\n",
    "#%   fit with power p and feature normalization (mu, sigma).\n",
    "\n",
    "\n",
    "\n",
    "    #% We plot a range slightly bigger than the min and max values to get\n",
    "    #% an idea of how the fit will vary outside the range of the data points\n",
    "    tmpx = np.arange(min_x - 15, max_x + 25,  0.05)\n",
    "    x = np.reshape(tmpx, (tmpx.shape[0],1))\n",
    "\n",
    "    #% Map the X values \n",
    "    X_pol = polyFeatures(x, p)\n",
    "    X_pol = (X_pol-mu) / sigma\n",
    "\n",
    "    # Add ones\n",
    "    X_pol = np.hstack((np.ones((X_pol.shape[0],1)), X_pol))\n",
    "             \n",
    "    # Plot\n",
    "    plt.plot(x, X_pol.dot(theta), '--', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training data and fit\n",
    "#figure(1);\n",
    "plt.plot(X, y, 'rx', markersize=10, linewidth=1.5)\n",
    "plotFit(min(X), max(X), mu, sigma, theta, p)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')\n",
    "plt.title ('Polynomial Regression Fit (lambda = %f)' % lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(error_train, error_val) = learningCurve(X_poly, y, X_poly_val, yval, lamda);\n",
    "\n",
    "plt.plot(np.array(np.arange(1,m+1)), error_train, np.array(np.arange(1,m+1)), error_val);\n",
    "plt.title('Polynomial Regression Learning Curve (lambda = %f)'% lamda)\n",
    "plt.legend(('Train', 'Cross Validation'))\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Error')\n",
    "#axis([0 13 0 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Polynomial Regression (lambda = %f)\\n\\n'% lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('# Training Examples\\tTrain Error\\tCross Validation Error\\n')\n",
    "for i in np.arange(0,m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f\\n'% (i+1, error_train[i], error_val[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 8: Validation for Selecting Lambda ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validationCurve(X, y, Xval, yval):\n",
    "#%VALIDATIONCURVE Generate the train and validation errors needed to\n",
    "#%plot a validation curve that we can use to select lambda\n",
    "#%   [lambda_vec, error_train, error_val] = ...\n",
    "#%       VALIDATIONCURVE(X, y, Xval, yval) returns the train\n",
    "#%       and validation errors (in error_train, error_val)\n",
    "#%       for different values of lambda. You are given the training set (X,\n",
    "#%       y) and validation set (Xval, yval).\n",
    "#%\n",
    "\n",
    "    #% Selected values of lambda (you should not change this)\n",
    "    tmplamda = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n",
    "    lambda_vec = np.reshape(tmplamda, (tmplamda.shape[0],1))\n",
    "\n",
    "    #% You need to return these variables correctly.\n",
    "    error_train = np.zeros((lambda_vec.shape[0], 1));\n",
    "    error_val = np.zeros((lambda_vec.shape[0], 1));\n",
    "\n",
    "    #% ====================== YOUR CODE HERE ======================\n",
    "    #% Instructions: Fill in this function to return training errors in \n",
    "    #%               error_train and the validation errors in error_val. The \n",
    "    #%               vector lambda_vec contains the different lambda parameters \n",
    "    #%               to use for each calculation of the errors, i.e, \n",
    "    #%               error_train(i), and error_val(i) should give \n",
    "    #%               you the errors obtained after training with \n",
    "    #%               lambda = lambda_vec(i)\n",
    "    #%\n",
    "    #% Note: You can loop over lambda_vec with the following:\n",
    "    #%\n",
    "    #%       for i = 1:length(lambda_vec)\n",
    "    #%           lambda = lambda_vec(i);\n",
    "    #%           % Compute train / val errors when training linear \n",
    "    #%           % regression with regularization parameter lambda\n",
    "    #%           % You should store the result in error_train(i)\n",
    "    #%           % and error_val(i)\n",
    "    #%           ....\n",
    "    #%           \n",
    "    #%       end\n",
    "    #%\n",
    "    #%\n",
    "\n",
    "    for i in np.arange(lambda_vec.shape[0]):\n",
    "        lamda = lambda_vec[i,0];\n",
    "            \n",
    "        theta = trainLinearReg(X, y, lamda);\n",
    "\n",
    "        trainError = linearRegCostFunction(theta, X, y, lamda)\n",
    "        #trainGrad = linearRegCostGradFunction(theta, X, y, lamda)\n",
    "        valError = linearRegCostFunction(theta, Xval, yval, lamda)\n",
    "        #valGrad = linearRegCostGradFunction(theta, Xval, yval, lamda)\n",
    "\n",
    "\n",
    "        error_train[i] = trainError;\n",
    "        error_val[i] = valError;\n",
    "\n",
    "    return (lambda_vec, error_train, error_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lambda_vec, error_train, error_val) = validationCurve(X_poly, y, X_poly_val, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lambda_vec, error_train, lambda_vec, error_val)\n",
    "plt.legend(('Train', 'Cross Validation'))\n",
    "plt.xlabel('lambda');\n",
    "plt.ylabel('Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('\\tlambda\\tTrain Error\\tValidation Error\\n')\n",
    "for i in np.arange(0,lambda_vec.shape[0]):\n",
    "    print('  \\t%f\\t%f\\t%f\\n'% (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
