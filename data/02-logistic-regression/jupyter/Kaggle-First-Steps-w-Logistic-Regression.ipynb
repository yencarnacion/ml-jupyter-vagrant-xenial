{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy and pandas, and DataFrame / Series\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import scipy.optimize as optimize\n",
    "import scipy\n",
    "# Set some numpy options\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set some pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# And some items for matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "# read http://scikit-image.org/docs/dev/user_guide.html \n",
    "# for documentation\n",
    "from skimage import io\n",
    "from skimage import color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"kaggle/trainLabels.csv\")\n",
    "m  = labels[\"ID\"].max()\n",
    "print (m)\n",
    "\n",
    "\n",
    "X = None\n",
    "img_size = 32\n",
    "for i in np.arange(1, m+1):\n",
    "    img = color.rgb2gray (io.imread(\"kaggle/train/{0}.Bmp\".format(i)))\n",
    "    img32 = scipy.misc.imresize(img, (img_size,img_size))\n",
    "    img32[img32<128] = 0\n",
    "    img32[img32>=128] = 1\n",
    "    img_array = np.array(img32).flatten()\n",
    "    img_array = img_array.reshape(1, img_array.shape[0])\n",
    "    if(X is None):\n",
    "        X = img_array\n",
    "    else:\n",
    "        X = np.vstack((X, img_array))\n",
    "    \n",
    "    #X = np.concatenate(([X,img32.flatten(1)]), axis=0)\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_s = labels[\"Class\"].unique()\n",
    "np.ndarray.sort(labels_s)\n",
    "num_labels = labels_s.shape[0]\n",
    "print(labels_s.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y = np.ones((m,1))*-1\n",
    "for i in np.arange(labels_s.shape[0]):\n",
    "    have_true = labels[\"Class\"]==labels_s[i]\n",
    "    for j in np.arange(m):\n",
    "        if(have_true[j] == True):\n",
    "            y[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None):\n",
    "#DISPLAYDATA Display 2D data in a nice grid\n",
    "#   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data\n",
    "#   stored in X in a nice grid. It returns the figure handle h and the \n",
    "#   displayed array if requested.\n",
    "\n",
    "    # Set example_width automatically if not passed in\n",
    "    if example_width == None:\n",
    "        example_width = round(math.sqrt(X.shape[1]));\n",
    "\n",
    "    # Compute rows, cols\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    example_height = round(n / example_width)\n",
    "    \n",
    "    # Compute number of items to display\n",
    "    display_rows = math.floor(math.sqrt(m));\n",
    "    display_cols = math.ceil(m / display_rows);\n",
    "\n",
    "    # Between images padding\n",
    "    pad = 1;\n",
    "    \n",
    "    # Setup blank display\n",
    "    display_array = - np.ones((pad + (display_rows * (example_height + pad)), pad + (display_cols * (example_width + pad))))\n",
    "\n",
    "    # Copy each example into a patch on the display array\n",
    "    curr_ex = 0;\n",
    "    for j in np.arange(display_rows):\n",
    "        for i in np.arange(display_cols):\n",
    "            if curr_ex > m : \n",
    "                break; \n",
    "            # Copy the patch\n",
    "\n",
    "            # Get the max value of the patch\n",
    "            max_val = np.amax(abs(X[curr_ex, :]));\n",
    "            display_array_startx = j * (example_width + pad) #pad + (j * (example_height + pad)) - 1\n",
    "            display_array_endx = ((j + 1) * (example_width + pad)) -1 # pad + (j * (example_height + pad))\n",
    "            display_array_starty = i * (example_height + pad) # pad + (i * (example_width + pad))\n",
    "            display_array_endy = ((i +1) * (example_height + pad)) - 1\n",
    "                    \n",
    "            de = (np.array(X[curr_ex, :]).reshape((example_width, example_height))) / max_val\n",
    "            #print de.shape\n",
    "            display_array[display_array_startx:display_array_endx, display_array_starty:display_array_endy] = (np.array(X[curr_ex, :]).reshape((example_width, example_height))) / max_val;\n",
    "            curr_ex = curr_ex + 1;\n",
    "        \n",
    "        if curr_ex > m:\n",
    "            break; \n",
    "\n",
    "\n",
    "    # Display Image\n",
    "    plt.imshow(display_array, extent = [0,100,0,100], aspect='auto', cmap=plt.get_cmap('gray'))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% Randomly select 100 data points to display\n",
    "rand_indices=np.random.permutation(m)\n",
    "sel = X[rand_indices[0:100], :]\n",
    "print(sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#SIGMOID Compute sigmoid functoon\n",
    "#   J = SIGMOID(z) computes the sigmoid of z.\n",
    "    \n",
    "    g = 1.0 / (1.0 + np.exp(-z));\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrCostFunction(theta_unraveled, X, y, lamda):\n",
    "#LRCOSTFUNCTION Compute cost for logistic regression with \n",
    "#regularization\n",
    "#   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using\n",
    "#   theta as the parameter for regularized logistic regression and the\n",
    "#   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "# Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Compute the cost of a particular choice of theta.\n",
    "#               You should set J to the cost.\n",
    "#               Compute the partial derivatives and set grad to the partial\n",
    "#               derivatives of the cost w.r.t. each parameter in theta\n",
    "#\n",
    "# Hint: The computation of the cost function and gradients can be\n",
    "#       efficiently vectorized. For example, consider the computation\n",
    "#\n",
    "#           sigmoid(X * theta)\n",
    "#\n",
    "#       Each row of the resulting matrix will contain the value of the\n",
    "#       prediction for that example. You can make use of this to vectorize\n",
    "#       the cost function and gradient computations. \n",
    "#\n",
    "# Hint: When computing the gradient of the regularized cost function, \n",
    "#       there're many possible vectorized solutions, but one solution\n",
    "#       looks like:\n",
    "#           grad = (unregularized gradient for logistic regression)\n",
    "#           temp = theta; \n",
    "#           temp(1) = 0;   % because we don't add anything for j = 0  \n",
    "#           grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "#\n",
    "    theta_u = theta_unraveled.copy()\n",
    "    theta_u = theta_u.flatten()\n",
    "    theta = np.reshape(theta_u, (theta_u.shape[0], 1))\n",
    "    \n",
    "    hypotesis = sigmoid(X.dot(theta))\n",
    "    tmp = 1/m*np.sum(-(y*np.nan_to_num(np.log(hypotesis)))-(1-y)*np.nan_to_num(np.log(1-hypotesis)))\n",
    "    J = tmp + (lamda/(2*m))*(theta[1:].T.dot(theta[1:]) ) \n",
    "    \n",
    "    \n",
    "    gradient_a = 1/m*(X.T.dot(hypotesis-y))\n",
    "    gradient_b = (lamda/m)*(theta)\n",
    "\n",
    "    gradient_b[0,0]=0\n",
    "    grad = gradient_a + gradient_b;\n",
    "# =============================================================\n",
    "    grad = grad.flatten()\n",
    "        \n",
    "    return (J[0,0], grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lamda):\n",
    "#ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "#the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "#corresponds to the classifier for label i\n",
    "#   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "#   logisitc regression classifiers and returns each of these classifiers\n",
    "#   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
    "#   to the classifier for label i\n",
    "\n",
    "# Some useful variables\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n+1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: You should complete the following code to train num_labels\n",
    "#               logistic regression classifiers with regularization\n",
    "#               parameter lambda. \n",
    "#\n",
    "# Hint: theta(:) will return a column vector.\n",
    "#\n",
    "# Hint: You can use y == c to obtain a vector of 1's and 0's that tell use \n",
    "#       whether the ground truth is true/false for this class.\n",
    "#\n",
    "# Note: For this assignment, we recommend using fmincg to optimize the cost\n",
    "#       function. It is okay to use a for-loop (for c = 1:num_labels) to\n",
    "#       loop over the different classes.\n",
    "#\n",
    "#       fmincg works similarly to fminunc, but is more efficient when we\n",
    "#       are dealing with large number of parameters.\n",
    "#\n",
    "# Example Code for fmincg:\n",
    "#\n",
    "#     % Set Initial theta\n",
    "#     initial_theta = zeros(n + 1, 1);\n",
    "#     \n",
    "#     % Set options for fminunc\n",
    "#     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "# \n",
    "#     % Run fmincg to obtain the optimal theta\n",
    "#     % This function will return theta and the cost \n",
    "#     [theta] = ...\n",
    "#         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
    "#                 initial_theta, options);\n",
    "#\n",
    "#    for i in np.arange(num_labels):\n",
    "    for i in np.arange(num_labels):\n",
    "        print(\"{0} of {1}\".format(i+1, num_labels))\n",
    "        initial_theta = np.zeros((X.shape[1], 1))  \n",
    "        options = {'maxiter': 1000}\n",
    "        # Try 'CG', 'BFGS', 'L-BFGS-B'\n",
    "        Result = optimize.minimize(fun = lrCostFunction, x0 = initial_theta, args = (X, y==i, lamda), method = 'L-BFGS-B',\n",
    "                                 #jac = lrCostGradFunction, options=options)\n",
    "                                 jac=True, options=options)\n",
    "\n",
    "        optimal_theta = Result.x\n",
    "        \n",
    "        if Result.success != True:\n",
    "           print(Result.message)\n",
    "        \n",
    "        all_theta[i,:] = optimal_theta.flatten()\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "#PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "#are in the range 1..K, where K = size(all_theta, 1). \n",
    "#  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "#  for each example in the matrix X. Note that X contains the examples in\n",
    "#  rows. all_theta is a matrix where the i-th row is a trained logistic\n",
    "#  regression theta vector for the i-th class. You should set p to a vector\n",
    "#  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "#  for 4 examples) \n",
    "\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    p = np.zeros((X.shape[0], 1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Complete the following code to make predictions using\n",
    "#               your learned logistic regression parameters (one-vs-all).\n",
    "#               You should set p to a vector of predictions (from 1 to\n",
    "#               num_labels).\n",
    "#\n",
    "# Hint: This code can be done all vectorized using the max function.\n",
    "#       In particular, the max function can also return the index of the \n",
    "#       max element, for more information see 'help max'. If your examples \n",
    "#       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "#       for each row.\n",
    "#       \n",
    "\n",
    "    p = np.argmax(X.dot(all_theta.T), axis=1)\n",
    "    p = p.reshape((m,1))\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('\\nTraining One-vs-All Logistic Regression...\\n')\n",
    "\n",
    "lamda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "lamda_vec = [0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "lamda_vec = [0.001]\n",
    "\n",
    "#lamda_vec= [0.1]\n",
    "acc = 0\n",
    "winning_theta = None\n",
    "winning_lamda = None\n",
    "winning_pred = None\n",
    "for la in lamda_vec:\n",
    "    print('Trying Lambda {0}\\n'.format(la))\n",
    "    all_theta = oneVsAll(X, y, num_labels, la)\n",
    "    pred = predictOneVsAll(all_theta, X)\n",
    "    acc_tmp = np.mean(pred == y) * 100\n",
    "    print('\\n>>>>>>\\nFor Lambda {0} Training Set Accuracy: \\n{1}\\n<<<<<\\n'.format(la,acc_tmp))\n",
    "    if acc_tmp > acc:\n",
    "        acc = acc_tmp\n",
    "        winning_theta = all_theta.copy()\n",
    "        winning_lamda = la\n",
    "        winning_pred = pred.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\nBest Training Set Accuracy with Lambda {0}: \\n{1}'.format(winning_lamda, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files_a = None\n",
    "for file in glob.glob(\"kaggle/test/*.Bmp\"):\n",
    "    n_s= file.split('/')[2].split('.')[0]\n",
    "    n = int(n_s)\n",
    "    if(files_a is None):\n",
    "        files_a = np.array(n)\n",
    "    else:\n",
    "        files_a = np.vstack((files_a, n))\n",
    "        \n",
    "np.ndarray.sort(files_a)       \n",
    "files_a = files_a.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEST = None\n",
    "img_size = 32\n",
    "for i in files_a:\n",
    "    img = color.rgb2gray (io.imread(\"kaggle/test/{0}.Bmp\".format(i)))\n",
    "    img32 = scipy.misc.imresize(img, (img_size,img_size))\n",
    "    img32[img32<128] = 0\n",
    "    img32[img32>=128] = 1\n",
    "    img_array = np.array(img32).flatten()\n",
    "    img_array = img_array.reshape(1, img_array.shape[0])\n",
    "    if(TEST is None):\n",
    "        TEST = img_array\n",
    "    else:\n",
    "        TEST = np.vstack((TEST, img_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = predictOneVsAll(winning_theta, TEST)\n",
    "print(labels_s[test_pred[0][0]], files_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'Class' : test_pred.flatten(), 'ID' : files_a}\n",
    "\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "check0 = []\n",
    "\n",
    "for i in files_a:\n",
    "    check0.append(y[i-1]==df[df[\"ID\"]==i].Class.values[0])\n",
    "\n",
    "    #print(y[j]==df[df[\"ID\"]==j+1])\n",
    "\n",
    "print (np.mean(check0) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.to_csv(\"kaggle/test/submission.csv\",index=False, header=True, cols=[\"ID\",\"Class\"])\n",
    "y_test = []\n",
    "\n",
    "for i in files_a:\n",
    "    y_test.append(labels_s[df[df[\"ID\"]==i].Class.values[0]])\n",
    "f = open(\"kaggle/test/submission.csv\", 'w')\n",
    "\n",
    "print(len(y_test))\n",
    "j = 0\n",
    "f.write(\"ID,Class\\n\")\n",
    "for i in files_a:\n",
    "    f.write(\"{0},{1}\\n\".format(i, y_test[j]))\n",
    "    j = j+1\n",
    "    \n",
    "f.close()\n",
    "\n",
    "f = open(\"kaggle/test/submission2.csv\", 'w')\n",
    "j = 0\n",
    "f.write(\"ID,Class\\n\")\n",
    "for i in files_a:\n",
    "    f.write(\"{0},{1}\\n\".format(files_a[j], labels_s[test_pred[j][0]]))\n",
    "    j = j+1\n",
    "    \n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#delete\n",
    "i = 9928\n",
    "\n",
    "img = color.rgb2gray (io.imread(\"kaggle/test/{0}.Bmp\".format(i)))\n",
    "img32 = scipy.misc.imresize(img, (img_size,img_size))\n",
    "img32[img32<128] = 0\n",
    "img32[img32>=128] = 1\n",
    "img_array = np.array(img32).flatten()\n",
    "img_array = img_array.reshape(1, img_array.shape[0])\n",
    "\n",
    "l_pred = predictOneVsAll(winning_theta, img_array)\n",
    "\n",
    "print(l_pred)\n",
    "print(labels_s)\n",
    "labels_s[l_pred[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X, y.flatten())\n",
    "y_sklearn_predictions = lr.predict(X)\n",
    "acc_tmp = np.mean(y_sklearn_predictions == y.flatten()) * 100\n",
    "print('\\n>>>>>>\\nFor sklearn Training Set Accuracy: \\n{0}\\n<<<<<\\n'.format(acc_tmp))\n",
    "y_sklearn_test_predictions = lr.predict(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'scikit' : labels_s[y_sklearn_test_predictions.flatten().astype(int)].flatten(), \n",
    "     'logistic': labels_s[test_pred.flatten()], 'ID' : files_a}\n",
    "\n",
    "df2 = pd.DataFrame(d)\n",
    "l_vs_s = np.mean(test_pred.flatten()-y_sklearn_test_predictions.flatten().astype(int)==0) * 100\n",
    "print(\"Logistic Regression is same as scikit logistic regression %{0:.2f} of time\".format(l_vs_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"kaggle/test/submission2-sklearn.csv\", 'w')\n",
    "j = 0\n",
    "f.write(\"ID,Class\\n\")\n",
    "for i in files_a:\n",
    "    f.write(\"{0},{1}\\n\".format(files_a[j], labels_s[y_sklearn_test_predictions.flatten().astype(int)[j]]))\n",
    "    j = j+1\n",
    "    \n",
    "f.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
