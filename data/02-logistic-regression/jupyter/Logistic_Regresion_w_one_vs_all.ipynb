{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy and pandas, and DataFrame / Series\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "# Set some numpy options\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set some pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# And some items for matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this part of the exercise\n",
    "input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "num_labels = 10;          # 10 labels, from 1 to 10   \n",
    "                          # (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 1: Loading and Visualizing Data =============\n",
    "###  We start the exercise by first loading and visualizing the dataset. \n",
    "###  You will be working with a dataset that contains handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.loadtxt('mlclass-ex3/X.csv', delimiter=',')\n",
    "y = np.loadtxt('mlclass-ex3/y.csv', delimiter=',')\n",
    "\n",
    "m = len(y)\n",
    "y=np.resize(y, (m,1))\n",
    "y.shape\n",
    "\n",
    "#% Randomly select 100 data points to display\n",
    "rand_indices=np.random.permutation(m)\n",
    "\n",
    "sel = X[rand_indices[1:100], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayData(X, example_width=None):\n",
    "#DISPLAYDATA Display 2D data in a nice grid\n",
    "#   [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data\n",
    "#   stored in X in a nice grid. It returns the figure handle h and the \n",
    "#   displayed array if requested.\n",
    "\n",
    "    # Set example_width automatically if not passed in\n",
    "    if example_width == None:\n",
    "        example_width = round(math.sqrt(X.shape[1]));\n",
    "\n",
    "    # Compute rows, cols\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    example_height = round(n / example_width)\n",
    "    \n",
    "    # Compute number of items to display\n",
    "    display_rows = math.floor(math.sqrt(m));\n",
    "    display_cols = math.ceil(m / display_rows);\n",
    "\n",
    "    # Between images padding\n",
    "    pad = 1;\n",
    "    \n",
    "    # Setup blank display\n",
    "    display_array = - np.ones((pad + (display_rows * (example_height + pad)), pad + (display_cols * (example_width + pad))))\n",
    "\n",
    "    # Copy each example into a patch on the display array\n",
    "    curr_ex = 0;\n",
    "    for j in np.arange(display_rows):\n",
    "        for i in np.arange(display_cols):\n",
    "            if curr_ex > m : \n",
    "                break; \n",
    "            # Copy the patch\n",
    "\n",
    "            # Get the max value of the patch\n",
    "            max_val = np.amax(abs(X[curr_ex, :]));\n",
    "            display_array_startx = j * (example_height + pad) #pad + (j * (example_height + pad)) - 1\n",
    "            display_array_endx = ((j + 1) * (example_height + pad)) -1 # pad + (j * (example_height + pad))\n",
    "            display_array_starty = i * (example_width + pad) # pad + (i * (example_width + pad))\n",
    "            display_array_endy = ((i +1) * (example_width + pad)) - 1\n",
    "                    \n",
    "            de = (np.array(X[curr_ex, :]).reshape((example_height, example_width))) / max_val\n",
    "            #print de.shape\n",
    "            display_array[display_array_startx:display_array_endx, display_array_starty:display_array_endy] = (np.array(X[curr_ex, :]).reshape((example_height, example_width ))).T / max_val;\n",
    "            curr_ex = curr_ex + 1;\n",
    "        \n",
    "        if curr_ex > m:\n",
    "            break; \n",
    "\n",
    "\n",
    "    # Display Image\n",
    "    plt.imshow(display_array, extent = [0,100,0,100], aspect='auto', cmap=plt.get_cmap('gray'))\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayData(sel);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ============ Part 2: Vectorize Logistic Regression ============\n",
    "    In this part of the exercise, you will reuse your logistic regression\n",
    "    code from the last exercise. You task here is to make sure that your\n",
    "    regularized logistic regression implementation is vectorized. After\n",
    "    that, you will implement one-vs-all classification for the handwritten\n",
    "    digit dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#SIGMOID Compute sigmoid functoon\n",
    "#   J = SIGMOID(z) computes the sigmoid of z.\n",
    "    \n",
    "    #print(z.shape)\n",
    "    #math.exp(-z)\n",
    "    g = 1.0 / (1.0 + np.exp(-z));\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lrCostFunction(theta_unraveled, X, y, lamda):\n",
    "#LRCOSTFUNCTION Compute cost for logistic regression with \n",
    "#regularization\n",
    "#   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using\n",
    "#   theta as the parameter for regularized logistic regression and the\n",
    "#   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "# Initialize some useful values\n",
    "    m = y.shape[0] # number of training examples\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Compute the cost of a particular choice of theta.\n",
    "#               You should set J to the cost.\n",
    "#               Compute the partial derivatives and set grad to the partial\n",
    "#               derivatives of the cost w.r.t. each parameter in theta\n",
    "#\n",
    "# Hint: The computation of the cost function and gradients can be\n",
    "#       efficiently vectorized. For example, consider the computation\n",
    "#\n",
    "#           sigmoid(X * theta)\n",
    "#\n",
    "#       Each row of the resulting matrix will contain the value of the\n",
    "#       prediction for that example. You can make use of this to vectorize\n",
    "#       the cost function and gradient computations. \n",
    "#\n",
    "# Hint: When computing the gradient of the regularized cost function, \n",
    "#       there're many possible vectorized solutions, but one solution\n",
    "#       looks like:\n",
    "#           grad = (unregularized gradient for logistic regression)\n",
    "#           temp = theta; \n",
    "#           temp(1) = 0;   % because we don't add anything for j = 0  \n",
    "#           grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "#\n",
    "    theta_u = theta_unraveled.copy()\n",
    "    theta_u = theta_u.flatten()\n",
    "    theta = np.reshape(theta_u, (theta_u.shape[0], 1))\n",
    "    \n",
    "    hypotesis = sigmoid(X.dot(theta))\n",
    "    tmp = 1/m*np.sum(-(y*np.nan_to_num(np.log(hypotesis)))-(1-y)*np.nan_to_num(np.log(1-hypotesis)))\n",
    "    J = tmp + (lamda/(2*m))*(theta[1:].T.dot(theta[1:]) ) \n",
    "    \n",
    "    \n",
    "    gradient_a = 1/m*(X.T.dot(hypotesis-y))\n",
    "    gradient_b = (lamda/m)*(theta)\n",
    "\n",
    "    gradient_b[0,0]=0\n",
    "    grad = gradient_a + gradient_b;\n",
    "# =============================================================\n",
    "    grad = grad.flatten()\n",
    "        \n",
    "    return (J[0,0], grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, num_labels, lamda):\n",
    "#ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "#the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "#corresponds to the classifier for label i\n",
    "#   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "#   logisitc regression classifiers and returns each of these classifiers\n",
    "#   in a matrix all_theta, where the i-th row of all_theta corresponds \n",
    "#   to the classifier for label i\n",
    "\n",
    "# Some useful variables\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    all_theta = np.zeros((num_labels, n+1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: You should complete the following code to train num_labels\n",
    "#               logistic regression classifiers with regularization\n",
    "#               parameter lambda. \n",
    "#\n",
    "# Hint: theta(:) will return a column vector.\n",
    "#\n",
    "# Hint: You can use y == c to obtain a vector of 1's and 0's that tell use \n",
    "#       whether the ground truth is true/false for this class.\n",
    "#\n",
    "# Note: For this assignment, we recommend using fmincg to optimize the cost\n",
    "#       function. It is okay to use a for-loop (for c = 1:num_labels) to\n",
    "#       loop over the different classes.\n",
    "#\n",
    "#       fmincg works similarly to fminunc, but is more efficient when we\n",
    "#       are dealing with large number of parameters.\n",
    "#\n",
    "# Example Code for fmincg:\n",
    "#\n",
    "#     % Set Initial theta\n",
    "#     initial_theta = zeros(n + 1, 1);\n",
    "#     \n",
    "#     % Set options for fminunc\n",
    "#     options = optimset('GradObj', 'on', 'MaxIter', 50);\n",
    "# \n",
    "#     % Run fmincg to obtain the optimal theta\n",
    "#     % This function will return theta and the cost \n",
    "#     [theta] = ...\n",
    "#         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...\n",
    "#                 initial_theta, options);\n",
    "#\n",
    "    for i in np.arange(1,num_labels+1):\n",
    "        print(\"{0} of {1}\".format(i, num_labels))\n",
    "        initial_theta = np.zeros((X.shape[1], 1))   \n",
    "        options = {'maxiter': 1000}\n",
    "        # Try 'CG', 'BFGS', 'L-BFGS-B'\n",
    "        Result = optimize.minimize(fun = lrCostFunction, x0 = initial_theta, args = (X, y==i, lamda), method = 'L-BFGS-B',\n",
    "                                 jac = True, options = options)\n",
    "        optimal_theta = Result.x\n",
    "        all_theta[i-1,:] = optimal_theta.flatten(1)\n",
    "    \n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('\\nTraining One-vs-All Logistic Regression...\\n')\n",
    "\n",
    "lamda = 0.1;\n",
    "all_theta = oneVsAll(X, y, num_labels, lamda);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 3: Predict for One-Vs-All ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictOneVsAll(all_theta, X):\n",
    "#PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "#are in the range 1..K, where K = size(all_theta, 1). \n",
    "#  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "#  for each example in the matrix X. Note that X contains the examples in\n",
    "#  rows. all_theta is a matrix where the i-th row is a trained logistic\n",
    "#  regression theta vector for the i-th class. You should set p to a vector\n",
    "#  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "#  for 4 examples) \n",
    "\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "\n",
    "# You need to return the following variables correctly \n",
    "    p = np.zeros((X.shape[0], 1))\n",
    "\n",
    "# Add ones to the X data matrix\n",
    "    X = np.hstack((np.ones((m,1)), X))\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Instructions: Complete the following code to make predictions using\n",
    "#               your learned logistic regression parameters (one-vs-all).\n",
    "#               You should set p to a vector of predictions (from 1 to\n",
    "#               num_labels).\n",
    "#\n",
    "# Hint: This code can be done all vectorized using the max function.\n",
    "#       In particular, the max function can also return the index of the \n",
    "#       max element, for more information see 'help max'. If your examples \n",
    "#       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "#       for each row.\n",
    "#       \n",
    "\n",
    "    p = np.argmax(X.dot(all_theta.T), axis=1)\n",
    "    p = p.reshape((m,1))\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "    return p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = predictOneVsAll(all_theta, X)\n",
    "\n",
    "print('\\nTraining Set Accuracy: \\n', np.mean(pred == y) * 100);\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
